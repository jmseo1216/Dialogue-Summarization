{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer, scoring\n",
    "import datasets\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge \n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig, AutoConfig, AutoModelForCausalLM, GPT2LMHeadModel\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cateto/korean-gpt-neox-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"../data/\", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정\n",
    "        \"model_name\": 'cateto/korean-gpt-neox-125M', # 불러올 모델의 이름을 사용자 환경에 맞게 지정\n",
    "        \"output_dir\": \"/data/ephemeral/home/checkpoint/decoder_only_1/\" # 모델의 최종 출력 값을 저장할 경로를 설정\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 100,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#','#PhoneNumber#', '#Address#', '#PassportNumber#', '#CarNumber#', '#SSN#', '#CardNumber#', '#Email#', '#DateOfBirth#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 3,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        # \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"wandb\" # (선택) wandb를 사용할 때 설정\n",
    "    },\n",
    "    # (선택)\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"wandb_repo\",\n",
    "        \"project\": \"project_name\",\n",
    "        \"name\": \"run_name\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"model ckt path\", # 사전 학습이 진행된 모델의 checkpoint를 저장할 경로를 설정\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 100,\n",
    "        \"num_beams\": 2,\n",
    "        \"batch_size\" : 32,\n",
    "        # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 구성 정보\n",
    "config_path = \"./config2_decoder_only1.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./config2_decoder_only1.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "pprint(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (선택)wandb config 설정\n",
    "loaded_config['wandb']['entity'] = \"jungminseo\"\n",
    "loaded_config['wandb']['name'] = \"Decoder_only_v1_model_cateto/korean-gpt-neox-125M\"\n",
    "loaded_config['wandb']['project'] = \"NLP_Dialogue_Summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb 설정 내용을 확인\n",
    "loaded_config['wandb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성\n",
    "class Preprocess:\n",
    "    def __init__(self,\n",
    "            bos_token: str,\n",
    "            eos_token: str,\n",
    "        ) -> None:\n",
    "\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    @staticmethod\n",
    "    # 실험에 필요한 컬럼을 가져옴\n",
    "    def make_set_as_df(file_path, is_train = True):\n",
    "        if is_train:\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_df = df[['fname','dialogue','summary']]\n",
    "            return train_df\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_df = df[['fname','dialogue']]\n",
    "            return test_df\n",
    "\n",
    "    # GPT 모델의 입력, 출력 형태를 맞추기 위해 전처리를 진행   ### 수정\n",
    "    def make_input(self, dataset, doc_tokenizer, sum_tokenizer, doc_max_length, sum_max_len, is_test = False):\n",
    "        if is_test:\n",
    "            # inference 시에는 document 만 주어지고, 마지막에 bos_token 을 붙여 생성 시작\n",
    "            dialogue_text = dataset['dialogue']\n",
    "            summary_test = dataset['summary']\n",
    "            # <pad> <pad> d_1 d_2 d_3 ... d_n <bos>\n",
    "            dialogue = [doc_tokenizer(dialogues, padding = 'max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for dialogues in dialogue_text.values]\n",
    "            labels = [[-100] * sum_max_len for _ in dialogue]\n",
    "\n",
    "            out = {\"input_ids\": dialogue, \"labels\": labels}\n",
    "            print(\"inference을 위한 데이터에서 tokenizing 된 input 형태\")\n",
    "            print(dialogue[-1])\n",
    "            print(doc_tokenizer.convert_ids_to_tokens(dialogue[-1]))\n",
    "\n",
    "        else:\n",
    "            dialogue_text = dataset['dialogue']\n",
    "            summary_test = dataset['summary']\n",
    "\n",
    "            dialogue = [doc_tokenizer(dialogues, padding = 'max_length', truncation=True, max_length=doc_max_length-1, add_special_tokens=True)['input_ids'] + [doc_tokenizer.bos_token_id] for dialogues in dialogue_text.values]\n",
    "            summary = [sum_tokenizer(summaries + sum_tokenizer.eos_token, padding= 'max_length', truncation=True, max_length=sum_max_len, add_special_tokens=True)['input_ids'] for summaries in summary_test.values]\n",
    "\n",
    "            tokenized_senetences = [dialogue + summary for (dialogue, summary) in zip(dialogue, summary)]\n",
    "\n",
    "            labels = [[-100] * len(dialogue) + summary for (dialogue, summary) in zip(dialogue, summary)]\n",
    "            labels = [[-100 if token == sum_tokenizer.pad_token_id else token for token in l] for l in labels]\n",
    "            out = {\"input_ids\": tokenized_senetences, \"labels\": labels}\n",
    "\n",
    "            # document 와 summary를 이어 붙여서 모델 학습에 사용. document 뒤에는 bos_token 을 붙여 생성 시작을 명시하고, summary 를 붙인 후 맨 뒤에는 eos_token 으로 생성의 끝을 명시\n",
    "            # document를 padding 할 때는 side를 left로 주고, summary를 padding 할 때는 side를 right 로 주어 연속된 문장이 생성될 수 있도록 함\n",
    "            # <pad> <pad> d_1 d_2 d_3 ... d_n <bos> s_1 s_2 ... s_m <eos>\n",
    "            print(\"학습을 위한 데이터에서 tokenizing 된 input 형태\")\n",
    "            print(tokenized_senetences[-1])\n",
    "            print(doc_tokenizer.convert_ids_to_tokens(tokenized_senetences[-1]))\n",
    "            print(\"학습을 위한 데이터에서 label의 형태\")\n",
    "            print(labels[-1])\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train에 사용되는 Dataset 클래스를 정의\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, dialogue, tokenizer):\n",
    "        self.dataset = dialogue\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx): # 해당하는 idx데이터를 모델에 입력가능한 형태로 가공하여 반환\n",
    "        input_ids = torch.LongTensor(self.dataset[\"input_ids\"][idx])\n",
    "        labels = torch.LongTensor(self.dataset[\"labels\"][idx])\n",
    "        attention_mask =input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        return dict(input_ids = input_ids,\n",
    "                    labels = labels,\n",
    "                    attention_mask = attention_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"input_ids\"])\n",
    "\n",
    "# Validation에 사용되는 Dataset 클래스를 정의\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, dialogue, tokenizer):\n",
    "        self.dataset = dialogue\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx): # 해당하는 idx데이터를 모델에 입력가능한 형태로 가공하여 반환\n",
    "        input_ids = torch.LongTensor(self.dataset[\"input_ids\"][idx])\n",
    "        labels = torch.LongTensor(self.dataset[\"labels\"][idx])\n",
    "        attention_mask =input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        return dict(input_ids = input_ids,\n",
    "                    labels = labels,\n",
    "                    attention_mask = attention_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"input_ids\"])\n",
    "\n",
    "# Test에 사용되는 Dataset 클래스를 정의\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력\n",
    "def prepare_train_dataset(config, preprocessor, data_path, doc_tokenizer, sum_tokenizer, doc_max_len, sum_max_len):\n",
    "    train_file_path = os.path.join(data_path,'train.csv')\n",
    "    val_file_path = os.path.join(data_path,'dev_50.csv')\n",
    "\n",
    "    # train, validation에 대해 각각 데이터프레임을 구축\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
    "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
    "\n",
    "    # tokenizing\n",
    "    tokenized_train  = preprocessor.make_input(train_data, doc_tokenizer, sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    tokenized_val = preprocessor.make_input(val_data, doc_tokenizer, sum_tokenizer, doc_max_len, sum_max_len)\n",
    "    print('-'*10, 'Load data complete', '-'*10,)\n",
    "\n",
    "    #tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "    #                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    #tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "    #                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    #tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "    #                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_train, doc_tokenizer)\n",
    "\n",
    "    #val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "    #                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    #val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "    #                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    #val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "    #                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(tokenized_val, doc_tokenizer)\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
    "    return train_inputs_dataset, val_inputs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(config, tokenizer, pred):\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # pred.predictions이 튜플인 경우와 아닌 경우를 처리.\n",
    "    if isinstance(pred.predictions, tuple):\n",
    "        logits = pred.predictions[0]\n",
    "    else:\n",
    "        logits = pred.predictions\n",
    "\n",
    "    preds = logits.argmax(-1)\n",
    "\n",
    "    dialogue_max_len = config['tokenizer']['encoder_max_len']\n",
    "    decoded_preds = tokenizer.batch_decode(preds[:, dialogue_max_len:], skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels[:, dialogue_max_len:], skip_special_tokens=True)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {decoded_preds[0]}\")\n",
    "    print(f\"GOLD: {decoded_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {decoded_preds[1]}\")\n",
    "    print(f\"GOLD: {decoded_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {decoded_preds[2]}\")\n",
    "    print(f\"GOLD: {decoded_labels[2]}\")\n",
    "\n",
    "    # ROUGE score 계산\n",
    "    metric = datasets.load_metric(\"rouge\")\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # ROUGE 결과를 추출합니다.\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return {\n",
    "        'Rouge-1': result.get('rouge1', 0.0),\n",
    "        'Rouge-2': result.get('rouge2', 0.0),\n",
    "        'Rouge-L': result.get('rougeL', 0.0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 trainer 클래스와 매개변수를 정의합니다.\n",
    "def load_trainer_for_train(config,generate_model,tokenizer, train_inputs_dataset,val_inputs_dataset):\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "    # set training args\n",
    "    training_args = TrainingArguments(\n",
    "                output_dir=config['general']['output_dir'], # model output directory\n",
    "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
    "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
    "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
    "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "                optim =config['training']['optim'],\n",
    "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "                evaluation_strategy=config['training']['evaluation_strategy'], # evaluation strategy to adopt during training\n",
    "                save_strategy =config['training']['save_strategy'],\n",
    "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
    "                fp16=config['training']['fp16'],\n",
    "                load_best_model_at_end=config['training']['load_best_model_at_end'], # 최종적으로 가장 높은 점수 저장\n",
    "                seed=config['training']['seed'],\n",
    "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
    "                logging_strategy=config['training']['logging_strategy'],\n",
    "                # predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
    "                # generation_max_length=config['training']['generation_max_length'],\n",
    "                do_train=config['training']['do_train'],\n",
    "                do_eval=config['training']['do_eval'],\n",
    "                report_to=config['training']['report_to'] # (선택) wandb\n",
    "            )\n",
    "\n",
    "    # (선택) wandb를 사용하기 위해 초기화 \n",
    "    wandb.init(\n",
    "        entity=config['wandb']['entity'],\n",
    "        project=config['wandb']['project'],\n",
    "        name=config['wandb']['name'],\n",
    "    )\n",
    "\n",
    "    # (선택) 모델 checkpoint를 wandb에 저장하도록 환경 변수를 설정\n",
    "    os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "    os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "    # EarlyStopping 기능\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
    "    print('-'*10, 'Make trainer', '-'*10,)\n",
    "\n",
    "    # Trainer 클래스를 정의\n",
    "    trainer = Trainer(\n",
    "        model=generate_model, # 사용자가 사전 학습하기 위해 사용할 모델\n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        eval_dataset=val_inputs_dataset,\n",
    "        compute_metrics = lambda pred: compute_metrics(config, tokenizer,pred),\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks = [MyCallback]\n",
    "    )\n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "QLoRA 적용시\n",
    "\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "    print('-'*10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-'*10,)\n",
    "    model_name = config['general']['model_name']\n",
    "\n",
    "    # model의 hyperparameter를 setting\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenizer를 불러옵니다.\n",
    "    doc_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    sum_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "\n",
    "    # 양자화 설정 (8비트)\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "    # 사전 학습된 모델을 양자화 설정과 함께 불러옵니다.\n",
    "    generate_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quantization_config  # 양자화 설정 적용\n",
    "    )\n",
    "\n",
    "    # Special tokens를 추가\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    doc_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    sum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # 사전에 special token을 추가했으므로 재구성 해줍니다.\n",
    "    generate_model.resize_token_embeddings(len(doc_tokenizer))\n",
    "\n",
    "    # 모델의 파라미터를 Freeze합니다.\n",
    "    for param in generate_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # LoRA 설정\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # rank\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        inference_mode=False,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    # LoRA를 모델에 적용합니다.\n",
    "    generate_model = get_peft_model(generate_model, lora_config)\n",
    "\n",
    "    # 모델을 디바이스에 할당합니다.\n",
    "    generate_model.to(device)\n",
    "\n",
    "    print(generate_model.config)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return generate_model, doc_tokenizer, sum_tokenizer\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "    print('-'*10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-'*10,)\n",
    "    model_name = config['general']['model_name']\n",
    "\n",
    "    # 모델의 하이퍼파라미터를 설정\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenizer를 불러오기\n",
    "    doc_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    sum_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "\n",
    "    # Special tokens를 추가\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    doc_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    sum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # 패딩 토큰이 없으면 추가\n",
    "    if doc_tokenizer.pad_token is None:\n",
    "        doc_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    if sum_tokenizer.pad_token is None:\n",
    "        sum_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "    # 사전에 special token을 추가했으므로 재구성 \n",
    "    generate_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "    generate_model.resize_token_embeddings(len(doc_tokenizer))\n",
    "\n",
    "    generate_model.to(device)\n",
    "\n",
    "    print(generate_model.config)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return generate_model, doc_tokenizer, sum_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # device 정의\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # 사용할 모델과 tokenizer를 불러오기\n",
    "    generate_model , doc_tokenizer, sum_tokenizer = load_tokenizer_and_model_for_train(config,device)\n",
    "    print('-'*10,\"tokenizer special tokens : \",tokenizer.special_tokens_map,'-'*10)\n",
    "\n",
    "    # 학습에 사용할 데이터셋을 불러오기\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token']) # decoder_start_token: str, eos_token: str\n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, doc_tokenizer, sum_tokenizer, config['tokenizer']['encoder_max_len'], config['tokenizer']['decoder_max_len'])\n",
    "\n",
    "    # Trainer 클래스를 불러오기\n",
    "    trainer = load_trainer_for_train(config, generate_model, doc_tokenizer, train_inputs_dataset,val_inputs_dataset)\n",
    "    trainer.train()   # 모델 학습을 시작\n",
    "\n",
    "    # (선택) 모델 학습이 완료된 후 wandb를 종료\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
